\subsection{\funcX{}}\label{subsec:funcX}
\funcX{} is a distributed FaaS platform that is designed to support the unique needs of scientific computing. It combines a reliable and easy-to-use cloud-hosted interface with the ability to securely execute functions on distributed endpoints deployed on various computing resources. \funcX{} supports many high performance computing systems and cloud platforms, can use three popular container technologies, and can expose access to heterogeneous and specialized computing resources. The developer interface to funcX is one of its key benefits. Creating a servable function is as easy as writing a python function. Invoking a remote function involves calling a function on an instance of the funcX client class and passing in arguments as required by the remote function.\\

A funcX endpoint is a logical entity that represents a compute resource. The endpoint is managed by an agent process that allows the funcX service to dispatch functions to that resource for execution. The agent handles authentication and authorization, provisioning of nodes on the compute resource, and monitoring and management. Administrators or users can deploy a funcX agent and register an endpoint for themselves and/or others, providing descriptive (e.g., name, description) metadata. Each endpoint is assigned a unique identifier for subsequent use.
\TODO{Add figure to compliment this paragraph.}\\

Behind the scenes, funcX uses a heterogeneous executor model based on the Parsl parallel scripting project~\cite{Parsl_paper}.  This architecture uses manager processes which run at a particular compute site. The managers are configured to use one of many different task execution providers such as HTCondor, Slurm, Torque, or Kubernetes.\\

With this architecture it is possible to launch tasks on any of these different environments using the same, simple invocation syntax. Resources on different HPCs can be accessed by simply changing the endpoint identifier. The endpoint's configuration has numerous settings to tune the endpoint's use of compute resources to the specific environment and the computational profile of the job at hand. This can be to configure workers to take advantage of small windows of CPU availability, or perhaps allow the workers to wait for a larger allocation to be available. In either event, the funcX service will cause the task to wait and execute as many tasks as it can when the workers are available.  This helps to match the job profiles against a wide variety of compute environments. The endpoint process itself is light weight and consumes minimal resources while awaiting new tasks to schedule on workers.\\

Dependencies required to execute the function can be setup in a number of ways. Developers can provide a command that will be executed on each worker prior to scheduling any tasks. Typically this would be a pip install command. Environments that support containerization through shifter or singularity can specify a container in the setup. This is easiest to administer, however it implies that all tasks running on that endpoint can only depend on these provided settings. Currently, the Kubernetes executor offers more sophisticated support for containers. Users may register a docker image with funcX and associate that image with a function. The Kubernetes executor will launch worker pods with the requested container as needed to support task invocations.
